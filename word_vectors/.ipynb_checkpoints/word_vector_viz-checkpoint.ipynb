{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94363c7d-1f36-46e7-b2c0-66168261c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the interactive Tools for Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "\n",
    "# Get tools to download files and load them \n",
    "import pickle\n",
    "import urllib.request\n",
    "from os.path import exists as check_path\n",
    "from os import makedirs\n",
    "\n",
    "# Get tools to performe analysis\n",
    "import numpy as np\n",
    "from heapq import heappushpop\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b6466d8-3112-405f-b103-260958d63650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files_from_github(file_target_dir):\n",
    "    main_url = 'https://raw.githubusercontent.com/Goussha/word-vector-visualization/master/'\n",
    "    if not check_path(file_target_dir):\n",
    "        makedirs(file_target_dir)\n",
    "    \n",
    "    urls = [main_url+'file{}.p'.format(x) for x in range(1,9)]\n",
    "    file_names = [file_target_dir+'file{}.p'.format(x) for x in range(1,9)]\n",
    "    for file_name, url in zip(file_names, urls):\n",
    "        if not check_path(file_name):\n",
    "            print (\"Downloading file: \",file_name)\n",
    "            filename, headers = urllib.request.urlretrieve(url, filename=file_name)\n",
    "        else:\n",
    "            print('Allready exists: {}'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d542a48-9985-4e20-b1a3-b2c8120eeb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vecfiles(file_target_dir):\n",
    "    word_dict_loded = {}\n",
    "    for file_num in range(1,9):\n",
    "        full_file_name = file_target_dir+'file{}.p'.format(file_num)\n",
    "        print('Loading file: {}'.format(full_file_name))\n",
    "        with open(full_file_name, 'rb') as fp:\n",
    "            data = pickle.load(fp)\n",
    "        word_dict_loded.update(data)\n",
    "    return word_dict_loded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bd09e01-a286-443d-8138-f11426f3b17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  ./tmp/file1.p\n",
      "Downloading file:  ./tmp/file2.p\n",
      "Downloading file:  ./tmp/file3.p\n",
      "Downloading file:  ./tmp/file4.p\n",
      "Downloading file:  ./tmp/file5.p\n",
      "Downloading file:  ./tmp/file6.p\n",
      "Downloading file:  ./tmp/file7.p\n",
      "Downloading file:  ./tmp/file8.p\n",
      "Loading file: ./tmp/file1.p\n",
      "Loading file: ./tmp/file2.p\n",
      "Loading file: ./tmp/file3.p\n",
      "Loading file: ./tmp/file4.p\n",
      "Loading file: ./tmp/file5.p\n",
      "Loading file: ./tmp/file6.p\n",
      "Loading file: ./tmp/file7.p\n",
      "Loading file: ./tmp/file8.p\n"
     ]
    }
   ],
   "source": [
    "file_target_dir = \"./tmp/\"\n",
    "\n",
    "#Download files\n",
    "download_files_from_github(file_target_dir)\n",
    "#Load files and create dict\n",
    "word_dict = load_word2vecfiles(file_target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076dc461-9041-4b81-8ef4-ef288602f87c",
   "metadata": {},
   "source": [
    "## cosine similarity\n",
    "- reflects the degree of similarity between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ab7f4f-d9c7-4b3e-a136-2a4797a57156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    distance = 0.0\n",
    "    epsilon = 1e-10 # prevent deviding by 0\n",
    "    dot = np.dot(u.T, v)\n",
    "    # Compute the L2 norm of u & v\n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    cosine_similarity = dot/((norm_u*norm_v)+epsilon)\n",
    "    return cosine_similarity    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eba7d6f-7972-4c30-baa2-01c87ee8d7d2",
   "metadata": {},
   "source": [
    "## most k similar\n",
    "- find the most similar word to the input word by calculating the cosine similarity between the word vector and the other word vectors and returning K most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b77811b7-8b9c-4153-938c-b63e99da2d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_k_similar(word_in, word_dict,k=1):\n",
    "    words = word_dict.keys()\n",
    "    word_vec = word_dict[word_in]\n",
    "    most_similars_heap = [(-100, '') for _ in range(k)]\n",
    "    for w in words:\n",
    "        if w==word_in:\n",
    "            continue\n",
    "        cosine_sim = cosine_similarity(word_vec, word_dict[w])\n",
    "        heappushpop(most_similars_heap, (cosine_sim, w))\n",
    "    most_similars_tuples = [tup for tup in most_similars_heap]\n",
    "    _, best_words = zip(*most_similars_tuples)\n",
    "    return best_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ef1b5-c335-4f83-8aca-e80e9b5dc66c",
   "metadata": {},
   "source": [
    "## doesn't match\n",
    "- takes a list of words and returns the word the doesnt match by comparing cousine similarities each word and all the other words and returning the words with the lowest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8035a5b2-fa2b-441d-8a89-0b6ab36c2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doesnt_match(words, word_dict):\n",
    "    dots_tot = []\n",
    "    for w in words:\n",
    "        dots = 0\n",
    "        for w2 in words:\n",
    "            if w2==w:\n",
    "                continue\n",
    "            v = word_dict[w]\n",
    "            u = word_dict[w2]\n",
    "            dots = dots + cosine_similarity(v, u)\n",
    "        dots.tot.append(dots)\n",
    "    return (words[np.argmin(dots_tot)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d486093-5502-4040-8c25-3805c669a147",
   "metadata": {},
   "source": [
    "## complete_analogy\n",
    "- To find the analogy between words, this function subtracks one word vector from the other, and then add the difference to the vector of the third word.\n",
    "- The difference between two word vectors represents the difference between their meaning, or the relationship between them, also known as their analogy.\n",
    "- By adding this difference to a different word vector, you can find a forth word that has the same relationship with word 3 as words 1 and 2 have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b0c02-3492-4012-842e-7cc8bc7d81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, word_dict):\n",
    "    \n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    # Get the word embeddings e_a, e_b and e_c (≈1-3 lines)\n",
    "    e_a, e_b, e_c = word_dict[word_a],word_dict[word_b],word_dict[word_c]\n",
    "    words = word_dict.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "\n",
    "    # to avoid best_word being one of the input words, skip the input words\n",
    "    # place the input words in a set for faster searching than a list\n",
    "    # We will re-use this set of input words inside the for-loop\n",
    "    input_words_set = set([word_a, word_b, word_c])  \n",
    "    # loop over the whole word vector set    \n",
    "    for w in words:        \n",
    "        # to avoid best_word being one of the input words, skip the input words\n",
    "        if w in input_words_set:\n",
    "            continue       \n",
    "        #Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
    "        cosine_sim = cosine_similarity(e_b - e_a, word_dict[w]- e_c)       \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "    return best_word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
